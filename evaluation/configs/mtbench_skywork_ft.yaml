seed: 0
trainer:
  accelerator: auto
  fast_dev_run: False
  logger: False
model:
  class_path: rlaihf.algorithms.SkyworkLlama
  init_args:
    model_name_or_path: "Skywork/Skywork-Reward-Llama-3.1-8B-v0.2"
    model_dtype: float16
    save_dir: "/home/zhaoyiz/projects/data/reward_dicts/mt-bench_skywork_sorted_avg_new"
    dialog_keys: ["conversation_a", "conversation_b"]
    label_key: "score"
data:
  class_path: rlaihf.datamodules.MtbenchCorrEvalDatamodule
  init_args:
    load_dataset_name: "lmsys/mt_bench_human_judgments"
    load_dataset_split: "human"
    tokenize_pipeline:
      class_path: rlaihf.tokenize_pipelines.TokenizePipelineV2
      init_args:
        raw_data_convert: rlaihf.data_conversion.MtbenchSortedDataConversionV2
    collate_fn:
      class_path: rlaihf.collators.StandardPaddingCollatorV3
      init_args:
        other_keys: ["score", "id", "model_a", "model_b"]
    batch_size: 4
    save_dataset_dir: "~/datasets"
    num_workers: 1
    holdout_model_id: 0 # Change in script
    keep_tie: False
